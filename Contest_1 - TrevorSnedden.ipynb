{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trevor Snedden  \n",
    "CYBV 471\n",
    "competition 1\n",
    "\n",
    "# <h1 style=\"text-align: center; \">Competition 1 </h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and device management\n",
    "\n",
    "Optional Intel Extension for Scikit-learn was used in this Notebook. References can be found at [Intel's](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.gqalpy) website. Further download and install documentation using Conda, pip, and Intel pipelines can be found at [pypi.org](https://pypi.org/project/scikit-learn-intelex/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE #help balance the training set\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added the `encoding` and `encoding_errors` args to fesolve the error \"'utf-8' codec can't decode byte 0x92 in position 1646: invalid start byte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"UNSW_NB15_testing-set.csv\")\n",
    "training_set = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
    "features = pd.read_csv(\"UNSW-NB15_features.csv\", encoding='utf-8', encoding_errors=\"replace\")\n",
    "print(features.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varify that there are no Null values in the datasets that can scew results. **x.isnull().sum()** to count the number of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set['attack_cat'].value_counts())\n",
    "print(f\"null value: {training_set.isnull().sum()}\") #none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set['attack_cat'].value_counts())\n",
    "print(f\"Null values: {test_set['attack_cat'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the unused\n",
    "train_df = training_set.drop(columns=['id', 'label'])\n",
    "test_df = test_set.drop(columns=['id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot-encode\n",
    "\n",
    "train_df_encoded = pd.get_dummies(training_set, columns=['proto', 'service', 'state'])\n",
    "test_df_encoded = pd.get_dummies(test_set, columns=['proto', 'service', 'state'])\n",
    "print(train_df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "align train and test set after one-hot to ensure both sets have same number of columns\n",
    "\n",
    "this ensure that any categorical features that exist in the training set but not in the test set are still accounted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_encoded, test_df_encoded = train_df_encoded.align(test_df_encoded, join='left', axis=1)\n",
    "#just to make sure no na\n",
    "test_df_encoded.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scaling and normalization is not always necessary for trees\n",
    "since trees split data by threshholds. \n",
    "\n",
    "- encoding is required though since trees can't operate wth categorical (non-numeric data) need to convert with label encoding or one hot encoding.\n",
    "\n",
    "First thoughts selected features:  \n",
    " Due to the tree automaticaly selecting key features, my feature selection will be later in the notebook after GridSearch is conducted.  \n",
    "\n",
    "\n",
    "An advantage of Random Forest trees are their ability of calculating **feature importance**. These feature importance are accessible to be able to see and determin key features to the models decisions. This allows for further data selection that can improve model performance. Since this is detecting faults the model can determin features that don't concern themselfs with detection. (fix this line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x and y(targ) for train and test\n",
    "x_train = train_df_encoded.drop(columns=['attack_cat']) #features\n",
    "y_train = train_df_encoded['attack_cat'] #targ attack cat\n",
    "\n",
    "x_test = test_df_encoded.drop(columns=['attack_cat'])\n",
    "y_test = test_df_encoded['attack_cat']#targs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter SMOTE into the training data to give the minority features dummy features to match the majority class. Normal in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "#apply smote\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "print(\"Class distribution after SMOTE:\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = x_train.columns\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use train_test_split to further split into validation and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80 20 split\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Model\n",
    "\n",
    "After looking at the data and seeing the large amount of 'normal' variables i decided to imagine attacks as faults in the system. With the thought I decided to use a random forest tree model using scikit-learn's RandomForestClassifier(). This also allows me to explore the functionality of Random Forest Trees.  \n",
    "\n",
    "Due to the nature of trees minimal data processing was required although one-hot-encoding was utilized to convert categorical values to a readable numerical value.  \n",
    "\n",
    "For tuning i decided to use a grid search to find best fit parameters.\n",
    "\n",
    "Use metrics such as accuracy, precision, recall, and F1-score to understand performance across the attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created RangomForestGridSearch Class for easier model tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestGridSearch:\n",
    "    def __init__(self, grid_params:dict, cv:int=5, n_jobs:int=-1, verbose:int=2, random_state:int=42):\n",
    "        \"\"\"\n",
    "        Initialize the RandomForestGridSearch class with hyperparmaeters, cross_validation and other configs\n",
    "\n",
    "        :param grid_params: Dictionary of hyperparameters for the grid search\n",
    "        :param cv: Number of cross_validation folds. ex. cv=5; 4 train, 1 validation.\n",
    "        :param n_jobs: number of parallel jobs (-1 means using all available cores).\n",
    "        :param verbose: Verbosity level for progress output\n",
    "        :param random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "#   define the grid\n",
    "        self.grid_params = grid_params\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.best_rf_model = None # to store best model during grid search\n",
    "        self.best_params = None\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        Perform grid search with cross_val on the training data\n",
    "\n",
    "        :param X_train: Training set (features)\n",
    "        :param Y_train: Training labels\n",
    "        \"\"\"\n",
    "\n",
    "        #initialize RF_model (not training)\n",
    "        RF_model = RandomForestClassifier(random_state= self.random_state)\n",
    "\n",
    "    #initialize gridsearch\n",
    "        grid_search= GridSearchCV(estimator=RF_model,\n",
    "                                param_grid=self.grid_params,\n",
    "                                cv=self.cv, #number of folds. 4train 1test\n",
    "                                n_jobs=self.n_jobs, #all available cores\n",
    "                                verbose=self.verbose  #print progress\n",
    "                                ) \n",
    "        \n",
    "        #perform the grid search\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "\n",
    "        #extract best params and model\n",
    "        self.best_rf_model = grid_search.best_estimator_ #best model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        print(f\"best score: {grid_search.best_score_}\\nbest_params: {grid_search.best_params_}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the best model found during grid search to make prediction on the new data\n",
    "\n",
    "        :param X: Feature set for prediciton  (val or test set)\n",
    "        :return: Predicted labels\n",
    "        \"\"\"\n",
    "        if self.best_rf_model is None:\n",
    "            raise ValueError(\"need to fit the model before making prediction\")\n",
    "        return self.best_rf_model.predict(X)\n",
    "    \n",
    "    def evaluate(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "\n",
    "        :param X: Feature set for evaluation\n",
    "        :y_true: True target labels for accuracy \n",
    "        :return: Accuracy of model on data\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def feature_importances(self, feature_names ,plot:bool=False):\n",
    "        \"\"\"\n",
    "        get and display the features\n",
    "        :param feature_names: list of feature names corresponding to the feature matrix\n",
    "        \"\"\"\n",
    "        if self.best_rf_model is None:\n",
    "            raise ValueError('Need to train model')\n",
    "        \n",
    "        #get feature importance\n",
    "        feature_ranking = self.best_rf_model.feature_importances_\n",
    "        #sort by importance\n",
    "        idx = np.argsort(feature_ranking)[::-1]\n",
    "\n",
    "        #print the features\n",
    "        print(\"Feaure Rankings\")\n",
    "        for f in range(len(feature_ranking)):\n",
    "            print(f\"{f+1}: Feature {feature_names[idx[f]]} ({feature_ranking[idx[f]]})\")\n",
    "        if plot:        \n",
    "            #plot feature_rankings\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.title(\"Feature Rankings\")\n",
    "            plt.barh(range(len(feature_ranking)), feature_ranking[idx], align=\"center\")\n",
    "            plt.yticks(range(len(feature_ranking,)), [feature_names[i] for i in idx])\n",
    "            plt.xlabel('Relativce Importance')\n",
    "            plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    #'n_estimators': [100,200,300], #trees\n",
    "    'n_estimators': [290,300,310], #trees\n",
    "    'max_depth': [9,10,11], #depth of trees\n",
    "    #'max_depth': [None, 10,20], #depth of trees\n",
    "    'min_samples_split': [9,10,11], #number of samples to split internal node\n",
    "    #'min_samples_split': [2,5,10], #number of samples to split internal node\n",
    "    'min_samples_leaf': [1,2,3], # min number of sample each leaf node\n",
    "}\n",
    "\n",
    "rf_grid_search = RandomForestGridSearch(grid_params=grid_params)\n",
    "\n",
    "#fit the model\n",
    "rf_grid_search.fit(x_train, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "rf_grid_search.evaluate(x_val, y_val)\n",
    "#make predictions on test set\n",
    "y_val_pred = rf_grid_search.predict(x_test)\n",
    "\n",
    "#get important features\n",
    "rf_grid_search.feature_importances(feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances and their corresponding feature names\n",
    "importances = rf_grid_search.best_rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "\n",
    "# Get the top features (those with importance >= 0.02)\n",
    "top_features = []\n",
    "feature_rank = []\n",
    "for f in range(x_train.shape[1]):  # Iterate over the number of features\n",
    "    if importances[indices[f]] >= 0.01:  # Only include features with importance >= 0.02\n",
    "        top_features.append(feature_names[indices[f]])\n",
    "        feature_rank.append(importances[indices[f]])\n",
    "        print(f\"{f + 1}. Feature {feature_names[indices[f]]} ({importances[indices[f]]})\")\n",
    "\n",
    "# Plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Top Feature Importances (Importance >= 0.02)\")\n",
    "plt.barh(range(len(top_features)), feature_rank, align=\"center\")\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 test and results\n",
    "\n",
    "### test with selected top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter X_train and X_val to include only the top features\n",
    "X_train_filtered = x_train[top_features[2:]]  # Keep only the top important features in training set\n",
    "X_val_filtered = x_val[top_features[2:]]      # Keep only the top important features in validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of 'id' and 'label' increased accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new RandomForest model\n",
    "new_rf_model = RandomForestClassifier(random_state=42, n_estimators=290, max_depth=11, min_samples_split=10, min_samples_leaf=2, class_weight='balanced') #balanced gives more support to the underrepresented data\n",
    "\n",
    "# Train the new model on the filtered feature set\n",
    "new_rf_model.fit(X_train_filtered, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the filtered validation set\n",
    "y_val_pred_filtered = new_rf_model.predict(X_val_filtered)\n",
    "\n",
    "# Evaluate accuracy on the filtered validation set\n",
    "val_accuracy_filtered = accuracy_score(y_val, y_val_pred_filtered)\n",
    "print(f\"Validation Accuracy with Top Features: {val_accuracy_filtered}\")\n",
    "\n",
    "print(classification_report(y_val, y_val_pred_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter X_test to include only the top features\n",
    "X_test_filtered = x_test[top_features[2:]]\n",
    "\n",
    "# Predict on the filtered test set\n",
    "y_test_pred_filtered = new_rf_model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "test_accuracy_filtered = accuracy_score(y_test, y_test_pred_filtered)\n",
    "print(f\"Test Accuracy with Top Features: {test_accuracy_filtered}\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred_filtered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy summary:\n",
    "Overall accuracy of 78% on test data.  \n",
    "recision: The proportion of true positives (correctly predicted) out of all predicted positives.  \n",
    "Recall: The proportion of true positives out of all actual positives.  \n",
    "F1-Score: The harmonic mean of precision and recall, balancing both metrics.  \n",
    "\n",
    "Macro Average:\n",
    "Precision = 0.48: On average, the model is 48% precise across all classes. The low precision for Analysis and Backdoor is dragging down the overall average.\n",
    "\n",
    "Recall = 0.49: On average, the model is correctly identifying 49% of the true instances for each class. This shows that, overall, the model is missing many instances in the minority classes.\n",
    "\n",
    "F1-Score = 0.46: The overall balance between precision and recall for all classes is still relatively low.\n",
    "\n",
    "Weighted Average:  \n",
    "Precision = 0.84: The overall model is precise for the dataset as a whole, heavily influenced by the large number of Normal and Exploits samples.\n",
    "\n",
    "Recall = 0.78: This reflects the overall recall, again driven mostly by the majority classes (Normal and Exploits).\n",
    "\n",
    "F1-Score = 0.79: A solid performance when weighted by the size of the classes, but still indicating that there’s room for improvement, particularly for the minority classes.\n",
    "\n",
    "#### Key Observations:  \n",
    "Improvement in Minority Classes:\n",
    "\n",
    "For the DoS class, recall has significantly improved from ~9% to 72%, indicating that SMOTE helped the model better identify DoS attacks.\n",
    "\n",
    "The Backdoor class also saw a slight improvement in recall from 0% to 19%, but precision is still very low, which indicates the model is struggling to distinguish Backdoor attacks from other types of attacks.\n",
    "\n",
    "The Analysis class still has very poor recall and precision, indicating that even with SMOTE, the model is having difficulty recognizing this class. This could be due to feature similarity with other classes or insufficient information in the features to differentiate it.\n",
    "\n",
    "Exploits and Normal Classes:\n",
    "\n",
    "The model performs well on the Exploits and Normal classes, though the recall for Exploits dropped compared to before. This could be due to the balancing caused by SMOTE, where more minority class samples were added, forcing the model to focus more on them and slightly reducing performance on the majority classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate roc curve for interpritation: \n",
    "\n",
    "AUC (Area Under the Curve): The AUC value for each class gives a good indication of how well the model is distinguishing between that class and the others. A perfect model will have an AUC of 1.0, while a model that randomly guesses will have an AUC of 0.5.\n",
    "\n",
    "Micro-average ROC: This curve gives an overall picture of the model's performance across all classes, treating each prediction equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_classes = len(set(y_train))  # Number of classes in your dataset\n",
    "\n",
    "# Binarize the labels for ROC computation\n",
    "y_test_bin = label_binarize(y_test, classes=list(set(y_train)))  # One-hot encode the test labels\n",
    "\n",
    "# nitialize dictionaries to hold False Positive Rate (FPR), True Positive Rate (TPR), and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_test_prob = new_rf_model.predict_proba(X_test_filtered)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_test_prob.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot curve using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'ROC curve for class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "# Plot micro-average ROC curve\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})',\n",
    "         linestyle=':', linewidth=4)\n",
    "\n",
    "# Plot settings\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guess\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) curves for multi-class classification')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "This was a good insight on how Random Forest Trees opporate and how to incorporate a GridSearch to find best params.  \n",
    "\n",
    "After applying SMOTE to handle class imbalance and then focusing on key top features, a Random Forest model was trained and evaluated. Using ROC (Receiver Operating Characteristic) curve to visualize how well the model distinguishes between each class, providing further insights into the model's performance.\n",
    "\n",
    "Key takeaways from this process:\n",
    "\n",
    "Improved Performance on Minority Classes:\n",
    "\n",
    "Applying SMOTE improved the recall for minority classes like DoS and Backdoor, though the model still struggles with precision for these classes.  \n",
    "The ROC curves for the minority classes gave a more granular view of how well the model distinguishes these minority classes from others.\n",
    "\n",
    "ROC Curves:\n",
    "\n",
    "computed ROC curves for each class, and the Area Under the Curve (AUC) values indicated how well the model distinguishes each class from the rest.\n",
    "The micro-average ROC curve provided an aggregate performance measure across all classes, which is especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "Despite using SMOTE, some classes such as Analysis and Backdoor continued to show lower performance, highlighting that further improvements are necessary.\n",
    "The ROC curve revealed that the model performs better on classes like Normal and Exploits, but struggles with minority classes, which are harder to classify even after balancing the data.\n",
    "\n",
    "Potential Improvements:\n",
    "\n",
    "Tuning Hyperparameters: Further hyperparameter tuning, especially with class weighting in Random Forest, could help further boost performance for minority classes.\n",
    "Advanced Models: Using more advanced models like XGBoost or Gradient Boosting might yield better results for multi-class, imbalanced datasets.\n",
    "Feature Engineering: Adding more meaningful features or refining existing ones could improve the model's ability to differentiate between the attack types, particularly for hard-to-classify groups.\n",
    "\n",
    "Overall Accuracy:\n",
    "\n",
    "The overall accuracy was around 78%, and while the model performs well on majority classes (Normal, Exploits), it still requires fine-tuning to balance precision and recall for minority classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
